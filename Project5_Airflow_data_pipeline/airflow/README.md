# 1. How to run 
> Open a terminal, run `/opt/airflow/start.sh` to start Airflow UI: 
> When all AWS credentials and Redshift setting have been created under `/Admin/Connections/` navigation, trigger the DAG (turn it on). 

> dag.py： a data pipeline Sparkify Amazon Redshift Data Warehouse ETL processes with data quality check on Airflow. 
> The operators class: four custom operators to perform tasks (StageToRedshiftOperator, LoadFactOperator, LoadDimensionOperator, LoadDimensionOperator)
> Helper class: SQL transformations


# 2. Aim
A music streaming startup, Sparkify, has grown their user base and song database even more and want to build a data pipeline to automation adn monitoring the Amazon Redshift data warehouse ETL process. Their data resides in S3. These JSON logs are on user activity on the app, as well as on the songs in their app.

* Create and load AWS access keys for connection
* Create a Redshift instance for Data Warehouse ETL process
* Process the data Redshift Data Warehouse
* Build an ETL pipeline that extracts data from S3
* Data quality check


# 3. Project Datasets
Sparkify's data resides in S3：

>  `Song data: s3://udacity-dend/song_data`

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

> song_data/A/B/C/TRABCEI128F424C983.json
> song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

`{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`


>  `Log data: s3://udacity-dend/log_data`
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

> log_data/2018/11/2018-11-12-events.json
> log_data/2018/11/2018-11-13-events.json


# 4. The purpose of this data pipeline in the context of the startup, Sparkify, and their analytical goals.

* The purpose of this pipeline:
> Organize and manage data from `diverse sources` (song and log datasets) for access and analysis.
> Understand the core concepts of Apache Airflow, and use custom operators to perform tasks such as staging the data, filling the data warehouse, and running checks on the data as the final step. 


# 5. Dimensional table design

* Dimensional table design: 
Build dimensional tables based on STAR Schema:

Four Dimention tables: 

> USERS: user_id, first_name, last_name, gender, level

> SONGS: song_id, title, artist_id, year, duration

> ARTISTS: artist_id, name, location, lattitude, longitude

> TIME: start_time, hour, day, week, month, year, weekday

FACT table works with dimension tables: 

> SONG_PLAYS: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

![Sparktify database](https://drive.google.com/file/d/1X9JOHZ-MOsE9hWirvlMnfC9_ovpB_G7r/view?usp=sharing)
![Sparktify Star Schema](https://github.com/yfocusy/DataEngineering-Udacity-Nanodegree/blob/master/Project5_Airflow_data_pipeline/airflow/Star%20Schema.png?raw=true)

* Database design Image link (if the image doesn't show up) : https://drive.google.com/file/d/1X9JOHZ-MOsE9hWirvlMnfC9_ovpB_G7r/view?usp=sharing


# 6. ETL pipeline.

* ETL pipeline: 
ETL stands for extract, transform and load:  

> The ETL pineline reads each data file from different data sources. 

> FACT table songplays: data can be extracted from songs_table and user log data (JOIN on song's duration and song's title.

> Transfrom timestamp: For example, extract timestamp into hour/day/weekday/month/year. 

> Then load is the process of write the data back to S3 for storeage. 

# 7. DAG 
> Task dependencies:
![DAG](https://raw.githubusercontent.com/yfocusy/DataEngineering-Udacity-Nanodegree/master/P5_airflow_data_pipeline.png)


